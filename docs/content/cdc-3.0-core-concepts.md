# Core Concepts

## Concept introduction

In the Flink CDC 3.0 framework, storage objects in external systems are unifiedly mapped to tables. 
Therefore, changes in the storage objects of external systems are also mapped to table changes. 
Similar to traditional table concepts, each table has a unique Table ID and its own table structure (Schema).

To be compatible with most external systems, the Table ID is represented by a 3-tuple, (namespace, schemaName, table). 
Connectors need to establish the mapping between Table ID and storage objects in external systems. For instance, a database table in MySQL/Doris is mapped to (null, database, table), and a topic in a message queue system such as Kafka is mapped to (null, null, topic).

The data types flowing in the Flink CDC 3.0 framework are referred to as Events, which represent the change events generated by a table. 
Each event is marked with the target Table ID for which the change occurred. Events are categorized into SchemaChangeEvents and DataChangeEvents, representing changes in table structure and data respectively.

Data Source Connector captures the changes in storage objects in the upstream system and converts them into events to be output to the synchronization task. 
It also provides a MetadataAccessor for the framework to read the metadata of the upstream system. The connector for the data sink is called the Data Sink. 
Upon receiving an event, the Data Sink applies the changes to the storage objects in the target system, achieving data synchronization with the upstream system. 
Additionally, MetadataApplier is used to apply metadata changes from the upstream system to the target system.

Since events flow from the upstream to the downstream in a pipeline manner, the data synchronization task is also referred to as a Data Pipeline. 
A Data Pipeline consists of a Data Source, Route, Transform and Data Sink. The transform can modify events to some extent, and the router can remap the table IDs corresponding to events.

## Data Source

Data Source is used to access metadata and read the changed data from external systems. A Data Source can read data from multiple tables simultaneously.

- Type: The type of the source, such as MySQL, Postgres.
- Name: The name of the source, which is user-defined (optional, with a default value provided).
- Other custom configurations for the source.

The ability to synchronize the entire library comes from the implementation of Flink source. After obtaining the user configuration, the Flink source connector needs to be able to send all data in a database downstream according to the specified data type.

## Data Sink

The Data Sink is used to modify metadata and write change data to external systems. A Data Sink can write to multiple tables simultaneously.

- Type: The type of the sink, such as MySQL or PostgreSQL.
- Name: The name of the sink, which is user-defined (optional, with a default value provided).
- Other custom configurations for the sink.

## Transform (Not supported in the MVP version)

Data transformation specifies how to transform the change data for each row in a given table:

- Source table: The ID of the source table, supports regular expressions.
- SQL expression: Evaluates an expression on the data in the source table.
- Description: A description of the transformation expression, which is user-defined (optional, with a default value provided).

```yml
transform:
  - source-table: mydb.app_order_.*
    projection: id, order_id, TO_UPPER(product_name)
    filter: id > 10 AND order_id > 100
    description: project fields from source table    	# Optional parameter for description purpose
  - source-table: mydb.web_order_.*
    projection: CONCAT(id, order_id) as uniq_id, *
    filter: uniq_id > 10
    description: add new uniq_id for each row        	# Optional parameter for description purpose
```

## Route

Routing, for each change event, specifies the target table ID of the event. The most typical application scenario is the merge of sub-databases and sub-tables, routing multiple upstream source tables to the same result table.

- source-table: source table id, supports regular expressions
- sink-table: Result table id, supports regular expressions
- description: Routing rule description, user-defined (optional, default value provided)

```yml
route:
  - source-table: mydb.app_order_.*
    sink-table: app-order-topic
    description: sync all sharding tables to one destination table
  - source-table: mydb.web_order
    sink-table: odsdb.ods_web_order
    description: sync table to one destination table with given prefix ods_
```

## Data Pipeline

- Name: The name of the pipeline, which will be submitted to the Flink cluster as the job name.
- Other advanced capabilities such as automatic table creation, schema evolution, etc., will be implemented.
